\chapter{Punteggi endogeni}
I punteggi endogeni utilizzano il contenuto di un documento. Possono essere statici o dinamici (quindi dipendere o meno dall'interrogazione). Ci occuperemo dei metodi più classici, che si basano su un'interrogazione espressa come \textit{bag of words}. In questo modello, un'interrogazione è semplicemente un insieme di termini: non ci sono operatori booleani. Di solito viene utilizzata un'interpretazione implicita \textit{disgiunta} (i documenti rilevanti sono quelli che contengono almeno uno dei termini), ma è anche possibile seguire un'interpretazione congiunta.

Il criterio più banale per assegnare un punteggio a un documento è quello di \textit{conteggio}: assegnamo a un documento il punteggio dato dalla somma dei conteggi dei termini dell'interrogazione che compaiono nel documento stesso. In questo modo, i documenti in cui i termini compaiono più di frequente avranno un punteggio elevato (questo perché se contengono con alta frequenza un termine desiderato devono essere più rilevanti). Se $c_{t, d}$ è il conteggio per un certo termine in un certo documento e \set{Q} è l'insieme dei termini contenuti nella query, in formule
\begin{equation}
	r(d) = \sum_{t \in Q}{c_{t, d}}
\end{equation}
Il conteggio è un metodo molto primitivo: innanzitutto si presta molto facilmente a manipolazione. Inoltre non tiene conto del fatto che alcuni termini occorrono con grande frequenza non perché rilevanti, ma perché altamente frequenti all'interno di ogni documento. Per esempio, nell'interrogazione "Romeo e Giulietta" il metodo di conteggio valuterà in maniera estremamente positiva documenti contenenti un gran numero di "e", ignorando il fatto che potrebbero essere semplicemente documenti molto lunghi riguardanti tutt'altro.

Per ovviare a questi inconvenienti sono stati sviluppati degli \textit{schemi di pesatura} che aggiustano il punteggio assegnato a ogni termine in ogni documento in modo da ovviare agli inconvenienti del conteggio.

Il primo e più classico metodo è TF/IDF (\textit{term frequency / inverse document frequency}) \cite{tfidf}. Esso normalizza il conteggio dividendolo per il massimo conteggio all'interno del documento o per la lunghezza del documento stesso, e inoltre lo attenua moltiplicandolo per l'inverso della frequenza del termine, o, più frequentemente, per il logaritmo del numero di documenti diviso per la frequenza (cioè per il logaritmo dell'inverso della frequenza in senso probabilistico). Se $l$ è la lunghezza de documento e $f$ è la frequenza di $t$ nella collezione documentale, in formule
\begin{equation}
	r(d) = \sum_{t \in Q} \frac{c_{t, d}}{l}\log\bigb{\frac{N}{f}}
\end{equation}
Come accennato, al posto di $l$ è possibile utilizzare il massimo conteggio di un termine che compare in $d$. Sono possibili molte altre varianti (come applicare un logaritmo a $c_{t, d}$).

Tornando all'esempio precedente, il termine "e" comparirà in quasi tutti i documenti, e verrà quindi pressoché ignorato dallo schema TF/IDF, dato che $N / f \approx 1$. La normalizzazione sulla lunghezza del documento, per contro, cerca di tenere conto del fatto che in documenti lunghi è naturale che un termine compaia più volte. Si noti che, comunque, il termine di conteggio compare in maniera lineare, ed è quindi facilmente soggetto a manipolazione.

Lo studio degli schemi di pesatura è parte arte, parte scienza e parte magia. Numerosi schemi sono stati inventati in maniera puramente euristica, e si sono dimostrati efficaci alla prova dei fatti. Uno degli schemi più celebri è BM25 (stando agli autori, il 25-esimo tentativo), uno schema di pesatura basato sul \textit{modello probabilistico}. Supponiamo di voler calcolare il punteggio associato a un documento $D$ la cui lunghezza è $l$, rispetto a una collezione documentale contenente documenti di lunghezza media $L$; BM25 pesa un termine come segue
\begin{equation}
	\texttt{score}(D) = \frac{(1 + k_1)c_{t, d}}{k_1((1 - b) + bl / L) + c_{t, d}}\log\bigb{\frac{N - f + 0.5}{f + 0.5}}
\end{equation}
dove $b$ e $k_1$ sono dei parametri liberi che devono essere tarati sulla collezione documentale. La parte dentro al logaritmo è una versione del punteggio IDF. Si noti che la formula non è più lineare in $c$. In effetti, la formula cresce monotonicamente in $c$, ma ha limite asintotico $k_1 + 1$ (quindi un numero eccessivo di ripetizioni del termine non influisce più di tanto).

Si noti che se $k_1 = 0$ la forma si riduce alla parte IDF, mentre per $k_1$ molto grande la formula è approssimativamente lineare in $c$ (nelle applicazioni reali $k_1$ è in genere tra 1 e 3).

Il termine $b$ serve a controllare l'influenza della lunghezza del documento rispetto alla lunghezza media.
