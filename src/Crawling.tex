\chapter{Crawling}
Il \textit{crawling} è l'attività di scaricamento delle pagine web. Un \textit{crawler} è un dispositivo software che visita, scarica e analizza i contenuti delle pagine web a partire da un insieme di URL dato, detto \textit{seme}. Il crawler esegue una visita seguendo i collegamenti ipertestuali contenuti nelle pagine analizzate.

Le pagine web durante il processo di crawl si dividono in tre:
\begin{itemize}
	\item L'insieme delle pagine \textit{visitate}, \set{V}, che sono già state scaricate e analizzate;
	\item La \textit{frontiera}, \set{F}, che è l'insieme delle pagine conosciute ma che non sono ancora state visitate;
	\item L'insieme \set{U} degli URL sconosciuti.
\end{itemize}
Le differenze tra l'attività di crawling e una banale visita all'interno di un grafo sono molto importanti, prima di tutto c'è il fatto che un crawl ha una dimensione ignota, non conosciamo $|V_G|$; secondariamente la frontiera è un enorme problema, in quanto la sua dimensione tende a crescere molto più velocemente dell'insieme dei visitati.

In generale l'operazione di crawling parte caricando il seme in frontiera e, finchè la frontiera non è vuota, viene estratto un URL dalla frontiera, secondo determinate politiche, l'URL viene visitato (e quindi scarica la pagina corrispondente), lo analizza derivandone nuovi URL tramite i collegamenti ipertestuali contenuti all'interno della pagina e sposta l'URL nell'insieme dei visitati. I nuovi URL vengono invece aggiunti alla frontiera se sono sconosciuti, e quindi non sono in \set{V \cup F}.

Diverse politiche di prioritizzazione della frontiera possono poi dare luogo ad approcci diversi al processo di crawling, posso per esempio estrarre prima degli URL a cui si arriva partendo da pagine che contengono determinate parole chiave.
\section{Il crivello}
Il crivello è la struttura dati di base di un crawler, questo accetta in ingresso URL potenzialmente da visitare e permette di prelevare URL pronti alla visita. Ogni URL viene estratto una e una sola volta in tutto il processo di crawling, indipendentemente da quante volte è stato inserito all'interno della struttura. In questo senso il crivello unisce le proprietà di un dizionario a quelle di una coda con priorità e rappresenta al tempo stesso la frontiera, l'insieme dei visitati e la coda di visita. Combinare questi aspetti in una sola struttura è un lavoro complesso ma permette risparmi notevoli dal punto di vista pratico\footnote{Si noti che è possibile riordinare ulteriormente gli URL \textit{dopo} l'uscita dal crivello}.

Una prima osservazione è che spesso, per mantenere l'informazione di quali URL sono stati già visti (\set{V \cup F}) è preferibile sostituire gli URL con delle \textit{firme}, cioè con il risultato del calcolo di $h(u)$, dove h è una funzione di hash definita sulle stringhe e restituisce una stringa di bit di dimensione arbitraria, per esempio 64 bit. Questo ha due grandi benefici:
\begin{itemize}
	\item Molti URL possono essere di grandi dimensioni, mantenerli salvati in memoria con un numero di bit scelto a priori (per esempio 64 bit), indipendentemente dalla loro reale dimensione, si rivela un risparmio di spazio affatto trascurabile.
	\item Uniformiamo le lunghezze degli URL a un valore standard.
\end{itemize}
Il drawback di una soluzione del genere è che accettiamo il fatto che vi siano delle collisioni, è dunque possibile che due URL diversi vengano mappati sullo stesso valore di hash. Questo fenomeno è inevitabile, però se abbiamo una funzione di hash che lavora su un numero di bit abbastanza grande, la probabilità d'incontrare una collisione sarà così bassa da essere trascurabile.

Github.com implementa una soluzione del genere, viene impiegato SHA-1 (funzione di hash a 160bit) per calcolare un hash dell'URL di ciascuna delle repository nei loro database, la probabilità di collisione è così bassa che è sostanzialmente impossibile. Adesso sembra che vogliano muoversi verso SHA-256.

Supponiamo ora di avere in memoria $n$ firme, la probabilità che una nuova firma collida con una di quelle esistenti è $n/u$, dove $u$ è la dimensione dell'universo delle possibili firme. Ad esempio nel caso di un sistema a 64 bit ($u = 2^{64}$) possiamo memorizzare 100 miliardi di URL con una probabilità di falsi positivi nell'ordine di $10^{11} / 2^{64} < 2^{37} / 2^{64} = 1 / 2^{27} < 1 / 10^8$, quindi avremo meno di un errore ogni 100 milioni di URL.

Anche un semplice dizionario di firme in memoria che rappresenta \set{V \cup F}, accoppiato a una coda o pila su disco che tiene traccia di \set{F}, è sufficiente per un'attività di crawling di piccole dimensioni. Per dimensioni più grandi è necessario ingegnarsi e impiegare delle strutture dati, in parte, o completamente su disco, che consentano di mantenere l'occupazione totale di memoria centrale costante.

Questo è un compromesso tipico delle attività di crawling - strutture che si espandono in memoria centrale proporzionalmente alla frontiera sono troppo fragili e quindi rischiano di mandare in crash il processo o di sovraccaricare il sistema di gestione della memoria virtuale.

Solitamente le strutture dati impiegate in ambiti di crawling importanti dovrebbero entrare in un processo di \textit{degrado grazioso}, riducendo le performance, ma senza interrompere all'improvviso il funzionamento del programma.
\section{I filtri di Bloom}
La prima struttura che vedremo con queste proprietà è il \textit{filtro di Bloom}. Un filtro di Bloom \cite{Bloom} è una semplicissima struttura dati probabilistica a falsi positivi che rappresenta un dizionario, cioè un insieme di elementi da un universo dato. Permette di aggiungere elementi all'insieme e chiedere se un elemento è presente o meno nell'insieme.

Un filtro di Bloom di universo \set{X} è rappresentato da un vettore $\vec{b}$ di $m$ bit e da $d$ funzioni di hash $f_0, \dots, f_{d - 1}$ da \set{X} in $m$. Per aggiungere un elemento al filtro è necessario calcolare i valori per tutte le $d$ funzioni di hash e mettere a 1 il bit $d_{f_i(x)}$ con $0 \leq i < d$. Per sapere se un elemento è presente all'interno del filtro è comunque necessario calcolare tutte le $d$ funzioni di hash, qualora esista $i \in [0, d) \st d_{f_i(x)} = 0$ allora il valore non è presente nella struttura, altrimenti la risposta è positiva.

Intuitivamente, ogni volta che un elemento viene aggiunto al filtro la conoscenza della presenza dell'elemento viene sparsa in $d$ bit a caso, che vengono interrogati quando è necessario sapere se quell'elemento è stato memorizzato: è però possibile che i $d$ bit siano stati messi a 1 a seguito di una serie d'inserimenti precedenti, quindi la risposta a un'interrogazione per un elemento che non è presente nell'insieme risulta essere ugualmente positiva. Questo implica che a causa di collisioni sulle varie funzioni di hash si possono avere \textit{falsi positivi}, questo significa che il filtro risponde positivamente, sebbene l'elemento non sia presente in struttura.

I filtri di Bloom sono chiamati in questa maniera perché sono molto utili come filtri per strutture dati più lente che stanno su disco. Se si prevede che la maggior parte delle richieste avrà risposta negativa, un filtro di Bloom può ridurre significativamente gli accessi alla struttura soggiacente; oltre a questo il filtro tende a rispondere molto velocemente a richieste che hanno risposta negativa, basta infatti che una sola delle posizioni indicate dalle funzioni di hash abbia bit a 0 per rispondere falso, mentre è necessario controllare tutte le posizioni e accedere alla struttura dati soggiacente nel caso in cui la risposta sia positiva.

Di fatto, i filtri di Bloom sono risultati estremamente pratici per mantenere insiemi di grandi dimensioni in memoria, in particolare quando le dimensioni delle chiavi sono significative (e.g. degli URL).

Andiamo ora a vedere qual è la probabilità di un falso positivo. Con un'analisi ragionevolmente precisa (quella che presenta Bloom in \cite{Bloom}) saremo in grado di fornire valori ottimi di $m$ e $d$ data la probabilità di falsi positivi desiderata e il massimo numero di elementi memorizzabili nel filtro. In questo modo saremo in grado di scegliere la struttura dati meno ingombrante per ottenere una probabilità di falsi positivi scelta a piacere.

\subsection{Dimostrazione dell'efficacia dei filtri di Bloom}
Per calcolare l'efficacia dei filtri di Bloom, come detto in precedenza, si considererà la probabilità di osservare un falso positivo. Una prima semplificazione lecita (seguendo l'analisi di \cite{Bloom}) è quella di andare a calcolare la probabilità di un positivo qualunque dopo $n$ inserimenti, che è ovviamente una maggiorazione del caso dei falsi positivi.
Supponiamo di avere un vettore di $m$ posizioni e $d$ funzioni di hash uniformemente distribuite e indipendenti. Dopo l'$n$-esimo inserimento, la probabilità che un bit sia 0 è data da:
\begin{equation}
	P[\vec{b}[i] = 0 \st N = n] = \bigb{\frac{m - 1}{m}}^{dn} = \bigb{1 - \frac{1}{m}}^{dn}
\end{equation}
Si ottiene un positivo quando tutte le posizioni controllate sono a 1, ciò avviene con probabilità
\begin{equation}
	\varphi = \bigb{1 - \bigb{1 - \frac{1}{m}}^{dn}}^d
\end{equation}
siccome $\bigb{1 + \alpha/n}^n \rightarrow e^{\alpha}$ per $n \rightarrow \infty$
\begin{equation}
	\varphi = \bigb{1 - \bigb{1 - \frac{1}{m}}^{-m \frac{-dn}{m}}}^d \sim \bigb{1 - e^{-\frac{dn}{m}}}^d
\end{equation}
Sia ora $p = e^{-\frac{dn}{m}}$, allora
\begin{align*}
	 & \ln(p) = -\frac{dn}{m} \\
	 & m\ln(p) = -dn          \\
	 & d = -\frac{m\ln(p)}{n}
\end{align*}
Voglio ora minimizzare la probabilità di ottenere un positivo $\varphi$, quindi prendo $\bigb{1 - e^{-\frac{dn}{m}}}^d$ e sostituisco $p$, quindi $(1 - p)^{-m/n \cdot \ln(p)}$.\\
Riscrivo come esponenziale:
\begin{equation}
	e^{\ln(1 - p)^{-m/n \cdot \ln(p)}} = e^{-m/n \cdot \ln(1 - p)\ln(p)}
\end{equation}
Faccio la derivata rispetto a $p$ che viene:
\begin{equation}
	-\frac{m}{n} \cdot \bigb{\frac{1}{p}\ln(1 - p) - \frac{1}{1 - p} \cdot \ln(p)} \cdot e^{-m/n \cdot \ln(1 - p)\ln(p)}
\end{equation}
Il punto stazionario è quello per cui la parte tra parentesi si annulla, l'esponenziale è sempre $> 0$ quindi:
\begin{align*}
	\frac{1}{p}\ln(1 - p) - \frac{1}{1 - p} \cdot \ln(p) & = 0              \\
	(1 - p) \cdot \ln(1 - p)                             & = p \cdot \ln(p)
\end{align*}
Se $1 - p = p$ allora $p = 1/2$, questa è l'unica soluzione, come prova del 9 si studi $g(p) = p\ln(p) - (1 - p)\ln(1 - p)$ e risulterà che agli estremi la funzione vale 0 dato che:
\begin{equation}
	\lim_{p \to 0}p \ln(p) = \lim_{p \to 0} \frac{\ln(p)}{1/p} = \lim_{p \to 0} \frac{1/p}{-1/p^2} = \lim_{p \to 0} -p = 0
\end{equation}
la derivata invece è $\dot{g(p)} = \ln(1 - p) + \ln(p) + 2$.

Chiaramente va a meno infinito in 0  e 1, ma in $p = 1/2$ è positiva, ed è l'unico punto di massimo (dato che la derivata seconda ha un solo zero in $p = 1/2$). Concludiamo che $g(p)$ ha esattamente un massimo e un minimo in $[0, 1)$, e quindi esattamente uno zero in $(0, 1)$.
Per concludere, se $p = 1/2$ allora $d = - \frac{m\ln(p)}{n} = \frac{m\ln2}{n}$ per quanto riguarda la probabilità di avere uno in tutti i punti che andiamo a controllare all'interno del vettore:
\begin{equation}
	\varphi = \bigb{1 - e^{-\frac{dn}{m}}}^d = \bigb{1 - e^{-\ln2}}^d = \bigb{1 - \frac{1}{2}}^d = 2^{-d}
\end{equation}
Alla fine, la probabilità di (falsi) positivi è minimizzata da $d \approx \frac{m\ln2}{n}$, e in tal caso la probabilità di un (falso) positivo è $2^{-d}$. Vale a dire che aumentando linearmente il numero delle funzioni di hash impiegate e il numero di bit della struttura a $m \approx dn/\ln2 \approx 1.44dn$, si ha una riduzione esponenziale del numero di (falsi) positivi che possono essere incontrati.
Passiamo ora a fare alcune osservazioni tecniche:
\begin{itemize}
	\item È abbastanza intuitivo, ed è possibile dimostrare, che per avere falsi positivi con probabililtà $2^{k}$ occorre utilizzare almeno $k$ bit per elemento. Quindi un filtro di Bloom perde 44\% in spazio rispetto al minimo possibile.
	\item In linea di principio il filtro di Bloom ha una modalità di accesso alla memoria pessima, a causa dei $d$ accessi casuali al vettore, che possono causare $d$ fallimenti in cache.
	\item Ciononostante, il dimensionamento ottimo di un filtro di Bloom è lineare nel numero di chiavi attese. Questo fa sì che se dividiamo le chiavi in $k$ segmenti utilizzando una funzione di hash e costruiamo un filtro per segmento, l'occupazione in spazio non aumenta.

	      Dimensionando $k$ in modo che i segmenti abbiano la dimensione di una o due linee di cache si può abbattere il numero di cache failures (questa implementazione è detta \textit{block}). In questo caso però l'approssimazione che abbiamo utilizzato perde di precisione; inoltre, la divisione delle chiavi in segmenti non è mai uniforme ma ha una distribuzione binomiale negativa. Questi fattori peggiorano la probabilità di errore \cite{Putze}.
	\item Dall'analisi che abbiamo effettuato, $1/2$ è anche la probabililtà di un bit a 0, quindi, come si diceva all'inizio del paragrafo, un filtro di Bloom è molto più efficace nel riportare il fatto che un elemento non è presente nel filtro piuttosto che a riportarne la presenza.
	\item In teoria per utilizzare un filtro di Bloom dobbiamo calcolare $d$ funzioni di hash diverse, il che può essere molto costoso in termini di tempo. In realtà Kirsch e Mitzenmacher hanno dimostrato che estraendo due numeri interi a 64 bit $a$ e $b$ tramite una funzione di hash, i numeri $ai + b$, $0 \leq i < d$ sono $d$ hash sufficienti a replicare l'analisi condotta utilizzando funzioni indipendenti e pienamente casuali.
	\item Se un filtro di Bloom viene utilizzato per rappresentare gli URL già visti, soddisfa pienamente le nostre richieste: utilizza una quantità di memoria centrale costante, è relativamente veloce e affidabile e degrada graziosamente, man mano che il vettore si riempie la probabilità di falsi positivi aumenterà fino a diventare 1.
\end{itemize}
\section{Crivelli basati su database NoSQL}
Un modo più sofisticato e con degrado più grazioso di implementare un crivello è quello di utilizzare un cosiddetto \textit{database NoSQL}, che consiste semplicemente in una struttura parzialmente su disco che permette di memorizzare coppie chiave/valore utilizzando una quantità limitata di memoria centrale.

Uno degli esempi classici di database NoSQL è il BerkeleyDB, che permette di memorizzare coppie/chiave valore in maniera non ordinata od ordinata tramite una hash table e un B-tree parzialmente su disco. La memoria centrale è utilizzata come cache per accelerare le operazioni su disco.

Un approccio più sistematico, implementato inizialmente a Google sotto il nome di BigTable, è l'LSM tree \cite{bigTable}.
BigTable è stato successivamente reimplementato come progetto open-source, LevelDB, che è poi stato utilizzato come base per altri database NoSQL come RocksDB, l'implementazione di Facebook, che è utilizzata da commoncrawler, un crawler open-source.

Gli LSM tree sono basati su un concetto relativamente semplice, ma necessitano di un'implementazione accurata che sfrutti parallelismo e concorrenza per essere efficienti.

Un LSM-tree è diviso in vari livelli, ognuno dei quali contiene un sottoinsieme delle coppie chiave/valore che si intende rappresentare. Ogni livello ha una dimensione di base che cresce di un fattore dato rispetto al livello precedente, ma ha una certa elasticità nel dimensionamento (può essere, ad esempio, grande il doppio rispetto alla sua dimensione di base).

Il primo livello è sempre in memoria centrale e ha dimensione limitata a priori, solitamente è implementato tramite un normale dizionario ordinato (RB-tree o B-tree).

I livelli successivi sono memorizzati sotto forma di \textit{log} e sono una successione immutabile di coppie chiave/valore ordinate.
Il primo aspetto importante di un LSM-tree è che una chiave può comparire in più livelli, il valore associato è quello che compare nel livello più alto in cui è possibile trovare la chiave. Un'interrogazione in lettura consiste quindi in una ricerca della chiave a partire dal primo livello, non appena la chiave viene trovata si sa il suo valore.

La parte interessante è quella di scrittura: la coppia chiave/valore viene inizialmente inserita nel primo livello. Se a questo punto il primo livello eccede la sua dimensione massima, si esegue l'operazione di \textit{scarico} in cui un gruppo di chiavi viene estratto dal primo livello e aggiunto al secondo, in modo da riportare il primo livello alla sua dimensione naturale.

A questo punto l'operazione di scarico continua ricorsivamente verso il basso fino a quando, se accade, anche l'ultimo livello eccede la propria dimensione massima, ed esegue un'operazione di scarico su un nuovo livello dell'albero.

Si noti che tutte le operazioni su disco avvengono sequenzialmente, e che tutti i dati memorizzati su disco sono \textit{immutabili}. Queste due caratteristiche rendono le fusioni estremamente efficienti nelle architetture moderne, e semplificano notevolmente la gestione degli accessi paralleli.

Per cancellare un'associazione chiave valore viene inserita una coppia con la stessa chiave e un valore arbitrario noto come \textit{lapide}; la tecnica è simile a quella utilizzata per le tabelle di hash. La lapide viene trattata come ogni altro valore, ma in fase di ricerca, se ne viene trovata una associata alla chiave il processo si ferma e il valore viene considerato assente dall'albero. Se una lapide arriva all'ultimo livello, viene scartata.

Ci sono a questo punto numerose e importanti questioni ingegneristiche e implementative da considerare:
\begin{itemize}
	\item Il formato in cui vengono memorizzati i livelli può non essere uniforme, dipende dal tipo di supporti di memorizzazione utilizzati; salvare dati efficacemente ed efficientemente su nastri richiede tecniche diverse rispetto a un processo di salvataggio su disco magnetico.
	\item Ogni livello può essere frammentato in file più piccoli per permettere di selezionare più liberamente le chiavi da fondere, e per rendere più semplice operare le fusioni in parallelo. In questo caso ogni chiave compare in un solo frammento.
	\item Ogni frammento può essere arricchito con un dizionario approssimato a falsi positivi a bassa precisione (come un filtro di Bloom) che evita l'accesso al file nel caso in cui la chiave che stiamo cercando non sia all'interno del file. La bassa precisione fa sì che l'occupazione in memoria del filtro non sia particolarmente rilevante.
	\item Ogni frammento può contenere un indice sparso che memorizza le posizioni di un sottoinsieme di chiavi campionate a intervalli regolari; in questo modo, in fase di ricerca è possibile identificare rapidamente la zona del frammento che potenzialmente contiene la chiave, per poi procedere con una ricerca binaria o lineare. La scelta della frequenza di campionamento consente di bilanciare lo spazio occupato dalla struttura e l'efficacia del processo di ricerca.
	\item Anche in assenza di fusioni di livelli, in generale in un LSM-tree vengono lasciati in esecuzione dei thread che si occupano di fare il \textit{compattamento} della struttura:
	      \begin{itemize}
		      \item Controllano che il numero di copie per chiave non sia eccessivo
		      \item Rimuovono eventuali lapidi in eccesso
	      \end{itemize}
	\item Infine, tutte le operazioni di fusione non vengono effettuate veramente durante gli inserimenti, ma piuttosto vengono svolte con continuità da processi concorrenti.
\end{itemize}
Ci sono anche soluzioni ibride, che reinseriscono parzialmente gli alberi bilanciati negli LSM tree per consentire di velocizzare le operazioni di ricerca. Questo tipo di tecnologia è in continua evoluzione, anche perché diverse implementazioni o politiche di aggiornamento possono essere adatte a diversi carichi di lavoro.

Si noti che al crescere della frontiera l'LSM-tree continua a occupare la stessa quantità di memoria centrale e non ha decrementi di precisione, però il crivello rallenta e occupa più memoria di massa.
\section{Un crivello offline}
Un modo meno \textit{responsive} ma molto più semplice di implementare il crivello che effettua una visita in ampiezza e richiede memoria centrale costante senza utilizzare strutture dati, consiste nel tenere traccia, in ogni istante, di tre file:
\begin{itemize}
	\item Un file Z di URL già visti (\set{V \cup F}), in ordine lessicografico.
	\item Un file F di URL ancora da visitare, quindi la frontiera, in ordine di scoperta.
	\item Un file A, di lunghezza limitata a priori, che accumula temporaneamente gli URL incontrati durante la visita.
\end{itemize}
All'inizio dell'attività di crawl, Z e F sono inizializzati utilizzando il seme, e A è vuoto. Durante il crawl, gli URL da visitare vengono estratti da F (eventualmente alterandone l'ordine secondo qualche politica), e i nuovi URL che vengono incontrati vengono accumulati in A.

Quando F è vuoto o quando A raggiunge la dimensione massima si procede a eseguire l'operazione di \textit{fusione}:
\begin{itemize}
	\item A viene ordinato (lessicograficamente) e deduplicato, il risultato è A'.
	\item Z e A' vengono fusi per ottenere un file Z' che andrà a rimpiazzare Z.
	\item Durante la fusione, gli URL che sono in A' ma non Z vengono accodati a F.
\end{itemize}
La fusione di Z e A' può procedere in maniera sequenziale perché i due file sono ordinati. È evidente che ogni URL che viene incontrato dal crawler viene accodato a F esattamente una volta, e cioè durante la fusione che avviene dopo la prima volta che compare in A.
Questo tipo di organizzazione non è particolarmente performante se viene effettuata sulla macchina che sta eseguendo l'attività di crawling, sebbene l'ordinamento di A si possa effettuare in memoria costante. Se però è possibile ordinare e fondere file utilizzando un framework di ordinamento distribuito, come MapReduce \cite{MapReduce}, o la sua implementazione open-source, Hadoop, le prestazioni possono essere molto migliorate, e la semplicità del codice può giocare a favore di questa scelta.

Va notato che l'ordinamento effettuato su A altera l'ordine di accodamento. Per recuperare l'effetto di una visita in ampiezza è necessario recuperare l'ordine originale degli URL. Questo risultato si può ottenere, per esempio, memorizzando in A, oltre agli URL, la posizione ordinale della loro prima occorrenza, e riordinando i nuovi URL scoperti in tale ordine prima di accodarli a F. È anche possibile tenere A quando si crea A', e durante la fusione mantenere invece di una lista di URL scoperti una lista di \textit{posizioni} in A di URL scoperti. A quel punto è sufficiente ordinare la lista di posizioni e scandirla in parallelo con A per estrarre sequenzialmente e nell'ordine di accodamento in A gli URL scoperti.

Infine, da un punto di vista pratico è conveniente mantenere in Z non gli URL già visti, ma le loro firme. Per fare funzionare correttamente il passo di fusione è però a questo punto necessario ordinare e deduplicare A utilizzando come chiavi le firme degli URL.

Si noti che al crescere della frontiera il crivello offline diventa più lento e occupa più memoria di massa, ma l'utilizzo di memoria centrale resta costante e non si hanno decrementi di precisione.
\section{Il crivello di Mercator}
Mercator è un crawler \cite{Mercator} il cui crivello è una versione parzialmente in memoria del crivello offline descritto in precedenza. Le firme degli elementi in A vengono mantenute in un vettore in memoria, evitando di eseguire ordinamenti su disco.

Il crivello è formato da un vettore $\vec{S}$, di dimensione fissata $n$, in memoria centrale che contiene firme di URL, inizialmente vuoto, ed è riempito incrementalmente. Su disco, invece, teniamo un file Z che contiene tutte le firme degli URL sinora mai incontrati e un file ausiliario A, inizialmente vuoto.

Ogni volta che un URL $u$ viene inserito nel crivello, aggiungiamo $h(u)$ a $\vec{S}$ e $u$ al file A. Il punto chiave è che cosa succede quando $\vec{S}$ raggiunge la massima dimensione; operiamo allora uno \textit{scarico} nel seguente modo:
\begin{enumerate}
	\item Ordino $\vec{S}$ indirettamente, cioè creo il vettore $\vec{V}$ contenente gli indici $i$ associati ai valori $\vec{S}$[i], ordino stabilmente $\vec{V}$ utilizzando come chiave le firme in $\vec{S}$. Al termine del processo, le firme $\vec{S}$[$\vec{V}$[i]] sono in ordine crescente al crescere di i\footnote{Si supponga per esempio che $\vec{S}$=\{C, F, B, A, A, E, D\}, allora il vettore associato a $\vec{S}$ sarà banalmente $\vec{V}$=\{0, 1, 2, 3, 4, 5, 6\}. Eseguendo l'ordinamento di $\vec{V}$ con le firme in $\vec{S}$ come chiave darà il seguente risultato (provare per credere) $\vec{V}$=\{3, 4, 2, 0, 6, 5, 1\}, che risulta effettivamente essere un ordinamento \textit{stabile} delle posizioni associate alle firme in $\vec{S}$.}.
	\item Deduplico $\vec{S}$, quindi elimino le occorrenze successive alla prima per ogni firma.
	\item Z' = Z $\cup$ $\vec{S}$ marchiamo utili le firme in $\vec{S}$ che non compaiono in Z.
	\item Scandiamo ogni entry di $\vec{S}$ e A in parallelo (sono entrambi ordinati) e inseriamo in F gli URL in A corrispondenti alle entry marcate come utili al passo precedente.
	\item $\vec{S}$ e A vengono svuotati e Z viene sostituito da Z'.
\end{enumerate}
Innanzitutto, si noti che Z, alla fine di uno scarico, contiene di nuovo le firme di tutti gli URL mai incontrati. Inoltre in output abbiamo dato tutti e soli gli URL la cui firma non era parte di Z, dunque si trattava di URL che non erano ancora stati visitati. Infine, gli URL in output sono stati ovviamente emessi nell'ordine di accodamento in A.
\section{Gestione dei quasi duplicati}
Durante l'attività di crawling è comune trovare pagine che sono quasi identiche (varianti dello stesso sito, calendari, immagini, etc\dots). In base al tipo di crawling (e quindi in base alle sue politiche), queste pagine andrebbero considerate duplicate e non ulteriormente elaborate.

Un modo semplice ma efficace di gestire il problema in memoria centrale è quello di analizzare una forma normalizzata del documento, rimuovendo marcatura, date e altri dati che sono standard ma che potrebbero risultare differenti sulla base di meccanismi automatici. Il documento dovrebbe poi essere memorizzato in un filtro di Bloom.

Metodi molto più sofisticati per la rilevazione dei duplicati possono essere utilizzati offline prima del processo di indicizzazione.

Un metodo efficace di gestione online del problema, che è stato utilizzato per qualche tempo dal crawler di Google \cite{simhashgoogle}, è quello di porre in un dizionario (eventualmente approssimato) uno hash generato dall'algoritmo di SimHash di Charikar \cite{SimHash}. L'algoritmo genera hash che sono simili (nel senso che hanno distanza di Hamming bassa) per pagine simili. In particolare, si può usare l'identità di SimHash come definizione di quasi-duplicato.

Per calcolare SimHash dobbiamo prima di tutto stabilire il numero $b$ di bit dello hash, e fissare una buona funzione di hash $h$ che mappa stringhe in hash di $b$ bit. A un maggiore numero di bit corrisponderà una nozione più accurata di somiglianza. A questo punto il testo della pagina, in forma normalizzata, viene trasformato in un insieme di segnali \set{S}: un modo banale è utilizzare le parole del testo come segnali, ma è più accurato considerare i cosiddetti \textit{shingles} (segmenti di testo di 3-5 caratteri).

A ogni segnale $s \in$ \set{S} associamo ora uno hash $h(s)$. Il SimHash del testo ha il bit $i$ ($0 \leq i < b$) impostato a 1 se e solo se:
\begin{equation}
	|\{s \in S \st h(s)[i] = 1\}| > |\{s \in S \st h(s)[i] = 0\}
\end{equation}
Due documenti che hanno lo stesso SimHash sono molto simili, e la somiglianza diventa sempre meno significativa se si permettono distanze di Hamming superiori. Si noti che è banale \textit{pesare} i segnali in modo che alcuni siano più importanti di altri.

Trovare elementi a breve distanza di Hamming è un problema interessante una cui soluzione pratica per distanze piccole è descritta è \cite{google}.

\section{Gestione della politeness}
Un altro dei problemi pratici che rende l'attività di crawling diversa da una semplice visita è la gestione della \textit{politeness}: non si dovrebbe eccedere nella quantità di tempo dedicato allo scaricamento da un singolo sito (pena, in genere, email furiose o taglio del traffico dal vostro IP).

Ci sono due modi fondamentali di operare questa limitazione:
\begin{itemize}
	\item Limitare il tempo tra una richiesta e l'altra.
	\item Limitare il rapporto tra il tempo di scaricamento e quello di non scaricamento.
\end{itemize}
Nel primo caso, dato un intervallo di tempo $t$, diciamo, quattro secondi, siamo costretti ad aspettare $t$ tra la fine di una richiesta e l'inizio della successiva per lo stesso sito. Nel secondo caso, data una frazione $p$ e un tempo di scaricamento massimo $s$ (diciamo, di un secondo) dobbiamo fare in modo che la proporzione tra il tempo di scaricamento e quello di non-scaricamento sia $p$. Si noti che questa condizione contempla anche una misurazione effettiva del tempo di scaricamento, dato che risorse particolarmente lente potrebbero richiedere un tempo maggiore di $s$.

La seconda soluzione è più interessante, perché permette di sfruttare una caratteristica della versione 1.1 del protocollo HTTP: è possibile cioè effettuare richieste multiple attraverso la stessa connessione TCP, evitando la (lenta) apertura e chiusura di una connessione per ogni risorsa scaricata. Le attività di scaricamento terminano non appena si supera la soglia $s$, con tempo di scaricamento effettivo $s'$, e a questo punto si aspetta per tempo $s'/p$, in maniera da forzare la gentilezza.
Per implementare questo tipo di politica, però è necessario alterare l'ordine di visita, dato che visitando gli URL nell'ordine in cui escono dal crivello si potrebbe incorrere in attese a vuoto consistenti.
\section{La coda degli host}
Un altro problema finora lasciato in parte è il ruolo della concorrenza. Certamente vorremo scaricare contemporaneamente da più siti: per farlo, possiamo istanziare molti flussi (sotto forma di migliaia di \textit{visiting thread}) di esecuzione che si occupano di scaricare i dati, e saranno quindi sempre occupati in attività di I/O. Le pagine scaricate possono essere poi analizzate da un gruppo di flussi più ridotto (i \textit{parsing thread}). Si noti che, al di là delle questioni di politeness, non possiamo permetterci che due flussi accedano allo stesso sito.

Questi problemi vengono risolti riorganizzando gli URL che escono dal crivello. Consideriamo una \textit{coda con priorità} contenente i siti noti al crawler. A ogni sito assegnamo come priorità il primo istante di tempo in cui sarà possibile scaricare dal sito senza violare la politeness. Si tratta di una coda di min-priorità, dunque in cima alla coda c'è il minimo.

Per ogni sito manteniamo una coda (FIFO nel caso della visita in ampiezza). Quando degli URL vengono emessi dal crivello, vengono accodati alla coda del sito cui appartengono.

Ogni flusso del crawler procede iterativamente nel seguente modo:
\begin{enumerate}
	\item Estrae il sito in cima alla coda (eventualmente aspettando il tempo necessario a far sì che questo sia scaricabile).
	\item Procede a scaricare una o più risorse.
	\item Riaccoda il sito aggiustando il timestamp di "readiness" secondo la politica di gestione della politeness.
\end{enumerate}
Se c'è un URL disponibile allo scaricamento, il sito deve essere già stato reso pronto per lo scaricamento prima del tempo corrente. Quindi o è in cima alla coda, o in cima alla coda c'è un sito che era pronto per lo scaricamento ancora prima. Il punto è che la cima della coda è sempre scaricabile.

Questo meccanismo rende automatica l'esclusività del download tra flussi: gli elementi della coda agiscono come \textit{token}, quando un elemento è in cima alla coda, il thread ha in possesso un token per scaricare da quel sito fino allo scadere del tempo.

Il costo della coda è logaritmico e l'aggiunta e la rimozione sono operazioni relativamente costose (al più polilogaritmiche) ma possono diventare problematiche in momenti di concorrenza intensa.

Infine, nel caso sia necessaria una politica di gentilezza da applicare agli indirizzi IP possiamo organizzare gli IP in una lista come quella descritta sopra. Rimanendo lungo il solco tracciato dalla metafora del token: quando un thread trova in cima alla lista un IP e un URL significa che il thread è in possesso del token per scaricare i dati associati da quell'URL e quello specifico indirizzo IP per una quantità limitata di tempo.
\section{Tecniche di programmazione concorrente lock-free}
Nella gestione delle strutture dati che abbiamo discusso è possibile che l'elevata concorrenza renda le strutture stesse poco efficienti. Se il numero di host è molto grande, per esempio, i flussi di scrittura e lettura potrebbero rimanere fermi per molto tempo ai punti di sincronizzazione o sui semafori che proteggono le zone di mutua esclusione.

è possibile alleviare il conflitto tra i flussi utilizzando delle tecniche di programmazione non bloccante, dette \textit{lock-free}. Una struttura dati lock-free non utilizza semafori: utilizza invece istruzioni hardware che permettono implementazioni efficienti. Quella che useremo è CAS (\textit{compare-and-swap}): dato un indirizzo $p$ e due valori $a$ e $b$, la CAS controlla che il valore memorizzato in $p$ sia $a$ e in questo caso lo sostituisce atomicamente con $b$, restituendo vero se la sostituzione è avvenuta.

Per dare un'idea delle tecniche lock-free, l'algoritmo 1 mostra come aggiungere concorrentemente un nodo a una lista con puntatori: l'implementazione è dovuta a Harris \cite{harris}.

La correttezza dell'algoritmo è banale, ma si noti anche che se l'istruzione CAS fallisce, un altro flusso ha eseguito l'inserimento: quindi, per ogni ciclo eseguito da uno dei flussi concorrenti c'è stato \textit{progresso} nell'aggiornamento della struttura. Detto altrimenti: non è possibile dire quante volte uno specifico flusso debba eseguire il ciclo prima di avere successo nell'inserimento, ma a ogni fallimento corrisponde un successo di un altro flusso.
\begin{algorithm}
	\caption{Algoritmo lock-free per aggiungere un nodo (n) a una lista (dopo p)}
	\begin{algorithmic}
		\Do
		\State $t \gets p.next$
		\State $n.next \gets t$
		\doWhile{!CAS(\&p.next, t, n)}
	\end{algorithmic}
\end{algorithm}
L'algoritmo per la cancellazione è più complesso: l'idea di utilizzare la stessa tecnica non funziona perché è possibile cancellare un nodo mentre sta venendo inserito un successore, cancellando così l'effetto dell'inserimento.

Si noti che non è richiesta nessuna sincronizzazione in lettura: la lista non è mai in uno stato incoerente, per cui può essere scandita tramite lo stesso algoritmo utilizzato per una normale lista collegata. Inoltre, in pratica, per evitare che in condizioni di elevata concorrenza il numero di volte che l'operazione CAS fallisce divenga troppo importante, si può utilizzare una tecnica di \textit{exponential backoff} che consiste nel mettere in attesa thread che falliscono per un tempo che ha una crescita esponenziale.

Molti linguaggi moderni offrono strutture lock-free built-in, come la \texttt{ConcurrentLinkedQueue} di Java, che implementa l'algoritmo di Michael e Scott \cite{prisonmike}.
